[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](https://github.com/linhuixiao/Awesome-Visual-Grounding/pulls)
<br />
<p align="center">
  <h1 align="center">Introduce to Visual Grounding: A Comprehensive Survey</h1>
  <p align="center">
    <b> T-PAMI, 2024 </b>
    <br />
    <a href="https://scholar.google.com.hk/citations?user=4rTE4ogAAAAJ&hl=zh-CN&oi=sra"><strong> Linhui Xiao </strong></a>
    ¬∑
    <a href="https://yangxs.ac.cn/home"><strong>Xiaoshan Yang </strong></a>
    ¬∑
    <a href="https://scholar.google.com.hk/citations?user=o_DllmIAAAAJ&hl=zh-CN"><strong>Yaowei Wang </strong></a>
    ¬∑
    <a href="https://scholar.google.com.hk/citations?user=hI9NRDkAAAAJ&hl=zh-CN"><strong>Changsheng Xu</strong></a>
  </p>

  <p align="center">
    <a href='https://arxiv.org/'>
      <img src='https://img.shields.io/badge/arXiv-PDF-green?style=flat&logo=arXiv&logoColor=green' alt='arXiv PDF'>
    </a>

[//]: # (    <a href='https://ieeexplore.ieee.org/document/10420487'>)
[//]: # (      <img src='https://img.shields.io/badge/TPAMI-PDF-blue?style=flat&logo=IEEE&logoColor=green' alt='TPAMI PDF'>)
[//]: # (    </a>)
[//]: # (  </p>)
<br />

This repo is used for recording, tracking, and benchmarking several recent visual grounding methods to supplement our [survey]().  
If you find any work missing or have any suggestions (papers, implementations, and other resources), feel free to [pull requests](https://github.com/linhuixiao/Awesome-Visual-Grounding/pulls).
We will add the missing papers to this repo as soon as possible.

### üî• Add Your Paper in our Repo and Survey!

- You are welcome to give us an issue or PR (pull request) for your open vocabulary learning work !

- Note that: Due to the huge paper in Arxiv, we are sorry to cover all in our survey. You can directly present a PR into this repo and we will record it for next version update of our survey.

[//]: # (- **Our survey will be updated in 2024.3.**)


### üî• New

[//]: # ([-] Our work is accepted by T-PAMI !!! üî•üî•üî•)

[//]: # ([-] We update GitHub to record the available paper by the end of **2024/1/10**.)

- We update GitHub to record the available paper by the end of **2024/7/30**.


### üî• Highlight!!

- A comprehensive survey for Visual Grounding, including Referring Expression Comprehension and Phrase Grounding.

- It includes the most recently Grounding Multi-modal Large Language model and Visual-Language Pre-trained model grounding transfer works. 

- We list detailed results for the most representative works and give a fairer and clearer comparison of different approaches.

- We provide a list of future research insights.


[https://github.com/TheShadow29/awesome-grounding](https://github.com/TheShadow29/awesome-grounding)

[Awesome-Open-Vocabulary](https://github.com/jianzongwu/Awesome-Open-Vocabulary)

# Introduction



# Summary of References

## Fully Supervised Setting

### A. Traditional CNN-based Methods

### B. Transformer-based Methods




| Year | Venue | Paper Title                                                                                                                                                                                                   | Paper Link                                                                                                                                    | Code / Project |
|------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------|
| 2021 | ICCV   | J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, ‚Äú**Transvg: End-to-end visual grounding with transformers**,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1769‚Äì1779. | [paper](http://openaccess.thecvf.com/content/ICCV2021/html/Deng_TransVG_End-to-End_Visual_Grounding_With_Transformers_ICCV_2021_paper.html) | [Code](https://github.com/djiajunustc/TransVG) 
|      |       |                                                                                                                                                                                                               |                                                                                                                                             |
|      |       |                                                                                                                                                                                                               |                                                                                                                                             |


### C. VLP-based Methods

### D. Grounded Representation Learning

### E. Grounding Multimodal LLMs





## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=linhuixiao/Awesome-Visual-Grounding&type=Date)](https://star-history.com/#linhuixiao/Awesome-Visual-Grounding&Date)




